{
  "content": [
    {
      "type": "text",
      "text": "# Backtracking line search\n\nIn (unconstrained) mathematical optimization, a backtracking line search is a line search method to determine the amount to move along a given search direction. Its use requires that the objective function is differentiable and that its gradient is known.\nThe method involves starting with a relatively large estimate of the step size for movement along the line search direction, and iteratively shrinking the step size (i.e., \"backtracking\") until a decrease of the objective function is observed that adequately corresponds to the amount of decrease that is expected, based on the step size and the local gradient of the objective function. A common stopping criterion is the Armijo–Goldstein condition.\nBacktracking line search is typically used for gradient descent (GD), but it can also be used in other contexts. For example, it can be used with Newton's method if the Hessian matrix is positive definite.\nMotivation\nGiven a starting position \\mathbf{x} and a search direction \\mathbf{p}, the task of a line search is to determine a step size \\alpha > 0 that adequately reduces the objective function f:\\mathbb R^n\\to\\mathbb R (assumed C^1 i.e. continuously differentiable), i.e., to find a value of \\alpha that reduces f(\\mathbf{x}+\\alpha\\,\\mathbf{p}) relative to f(\\mathbf{x}). However, it is usually undesirable to devote substantial resources to finding a value of \\alpha to precisely minimize f. This is because the computing resources needed to find a more precise minimum along one particular direction could instead be employed to identify a better search direction. Once an improved starting point has been identified by the line search, another subsequent line search will ordinarily be performed in a new direction. The goal, then, is just to identify a value of \\alpha that provides a reasonable amount of improvement in the objective function, rather than to find the actual minimizing value of \\alpha.\nThe backtracking line search starts with a large estimate of \\alpha and iteratively shrinks it. The shrinking continues until a value is found that is small enough to provide a decrease in the objective function that adequately matches the decrease that is expected to be achieved, based on the local function gradient \\nabla f(\\mathbf{x})\\,.\nDefine the local slope of the function of \\alpha along the search direction \\mathbf{p} as m = \\nabla f(\\mathbf{x})^{\\mathrm T}\\,\\mathbf{p}=\\langle\\nabla f(\\mathbf{x}),\\mathbf{p}\\rangle  (where \\langle\\cdot,\\cdot\\rangle denotes the dot product). It is assumed that \\mathbf{p} is a vector for which some local decrease is possible, i.e., it is assumed that m .\nBased on a selected control parameter c\\,\\in\\,(0,1), the Armijo–Goldstein condition tests whether a step-wise movement from a current position\n\\mathbf{x} to a modified position \\mathbf{x}+\\alpha\\,\\mathbf{p} achieves an adequately corresponding decrease in the objective function. The condition is fulfilled, see , if f(\\mathbf{x}+\\alpha\\,\\mathbf{p})\\leq f(\\mathbf{x})+\\alpha\\,c\\,m\\,.\nThis condition, when used appropriately as part of a line search, can ensure that the step size is not excessively large. However, this condition is not sufficient on its own to ensure that the step size is nearly optimal, since any value of \\displaystyle \\alpha that is sufficiently small will satisfy the condition.\nThus, the backtracking line search strategy starts with a relatively large step size, and repeatedly shrinks it by a factor \\tau\\,\\in\\,(0,1) until the Armijo–Goldstein condition is fulfilled.\nThe search will terminate after a finite number of steps for any positive values of c and \\tau that are less than 1.  For example, Armijo used  for both c and \\tau in .\nAlgorithm\nThis condition is from . Starting with a maximum candidate step size value \\alpha_0>0\\,, using search control parameters \\tau\\,\\in\\,(0,1) and c\\,\\in\\,(0,1), the backtracking line search algorithm can be expressed as follows:\n# Set t = -c\\,m and iteration counter j\\,=\\,0.\n# Until the condition is satisfied that f(\\mathbf{x})-f(\\mathbf{x}+\\alpha_j\\,\\mathbf{p})\\geq \\alpha_j\\,t, repeatedly increment j and set \\alpha_j=\\tau\\,\\alpha_{j-1}\\,.\n# Return \\alpha_j as the solution.\nIn other words, reduce \\alpha_0 by a factor of \\tau\\, in each iteration until the Armijo–Goldstein condition is fulfilled.\nFunction minimization using backtracking line search in practice\nIn practice, the above algorithm is typically iterated to produce a sequence \\mathbf{x}_n, n = 1, 2, ..., to converge to a minimum, provided such a minimum exists and \\mathbf{p}_n is selected appropriately in each step. For gradient descent, \\mathbf{p}_n is selected as -\\nabla f(\\mathbf{x}_n).\nThe value of \\alpha_j for the j that fulfills the Armijo–Goldstein condition depends on \\mathbf{x} and \\mathbf{p}, and is thus denoted below by \\alpha (\\mathbf{x},\\mathbf{p}). It also depends on f, \\alpha_0, \\tau and c of course, although these dependencies can be left implicit if they are assumed to be fixed with respect to the optimization problem.\nThe detailed steps are thus, see , :\n# Choose an initial starting point \\mathbf{x}_0 and set the iteration counter n=0.\n# Until some stopping condition is satisfied, choose a descent direction \\mathbf{p}_n, update the position to \\mathbf{x}_{n+1}=\\mathbf{x}_n+\\alpha (\\mathbf{x}_n,\\mathbf{p}_n)\\,\\mathbf{p}_n, and increment n.\n# Return \\mathbf{x}_{n} as the minimizing position and f(\\mathbf{x}_n) as the function minimum.\nTo assure good behavior, it is necessary that some conditions must be satisfied by \\mathbf{p}_n. Roughly speaking \\mathbf{p}_n should not be too far away from  \\nabla f(\\mathbf{x}_n). A precise version is as follows (see e.g. ). There are constants C_1,C_2>0 so that the following two conditions are satisfied:\n# For all n, \\|\\mathbf{p}_n\\|\\geq C_1 \\,\\|\\nabla f(\\mathbf{x}_n)\\|. Here, \\|y\\| is the Euclidean norm of y. (This assures that if \\mathbf{p}_n=0, then also \\nabla f(\\mathbf{x}_n)=0. More generally, if \\lim _{n\\rightarrow\\infty}\\mathbf{p}_n=0, then also \\lim _{n\\rightarrow\\infty}\\nabla f(\\mathbf{x}_n)=0.) A more strict version requires also the reverse inequality: \\|\\mathbf{p}_n\\|\\leq C_3 \\,\\|\\nabla f(\\mathbf{x}_n)\\| for a positive constant C_3>0.\n# For all n,  \\|\\mathbf{p}_n\\|\\,\\|\\nabla f(\\mathbf{x}_n)\\| \\leq -C_2 \\,\\langle\\mathbf{p}_n,\\nabla f(\\mathbf{x}_n)\\rangle . (This condition ensures that the directions of \\mathbf{p}_n and -\\nabla f(\\mathbf{x}_n) are similar.)\nLower bound for learning rates\nThis addresses the question whether there is a systematic way to find a positive number \\beta (\\mathbf{x},\\mathbf{p}) - depending on the function f, the point \\mathbf{x} and the descent direction \\mathbf{p}  - so that all learning rates \\alpha \\leq \\beta (\\mathbf{x},\\mathbf{p}) satisfy Armijo's condition. When \\mathbf{p}=-\\nabla f(\\mathbf{x}), we can choose  \\beta (\\mathbf{x},\\mathbf{p})  in the order of 1/L(\\mathbf{x})\\,, where L(\\mathbf{x})\\, is a local Lipschitz constant for the gradient \\nabla f\\, near the point \\mathbf{x} (see Lipschitz continuity). If the function is C^2, then L(\\mathbf{x})\\, is close to the Hessian of the function at the point \\mathbf{x}. See  for more detail.\nUpper bound for learning rates\nIn the same situation where \\mathbf{p}=-\\nabla f(\\mathbf{x}), an interesting question is how large learning rates can be chosen in Armijo's condition (that is, when one has no limit on \\alpha_0 as defined in the section \"Function minimization using backtracking line search in practice\"), since larger learning rates when \\mathbf{x}_n is closer to the limit point (if exists) can make convergence faster. For example, in Wolfe conditions, there is no mention of \\alpha_0 but another condition called curvature condition is introduced.\nAn upper bound for learning rates is shown to exist if one wants the constructed sequence \\mathbf{x}_n converges to a non-degenerate critical point, see : The learning rates must be bounded from above roughly by ||H||\\times ||H^{-1}||^2. Here H is the Hessian of the function at the limit point, H^{-1} is its inverse, and ||.|| is the norm of a linear operator. Thus, this result applies for example when one uses Backtracking line search for Morse functions. Note that in dimension 1,  H is a number and hence this upper bound is of the same size as the lower bound in the section \"Lower bound for learning rates\".\nOn the other hand, if the limit point is degenerate, then learning rates can be unbounded. For example, a modification of backtracking line search known as unbounded backtracking gradient descent (see ) allows the learning rate to be half the size ||\\nabla f(\\mathbf{x}_n)||^{-\\gamma}, where 1>\\gamma >0 is a constant. Experiments with simple functions such as f(x,y)=x^4+y^4 show that unbounded backtracking gradient descent converges much faster than the basic version described in the section \"Function minimization using backtracking line search in practice\".\nTime efficiency\nAn argument against the use of Backtracking line search, in particular in Large scale optimisation, is that satisfying Armijo's condition is expensive. There is a way (so-called Two-way Backtracking) to go around, with good theoretical guarantees and has been tested with good results on deep neural networks, see . (There, one can find also good/stable implementations of Armijo's condition and its combination with some popular algorithms such as Momentum and NAG, on datasets such as Cifar10 and Cifar100.) One observes that if the sequence \\mathbf{x}_{n} converges (as wished when one makes use of an iterative optimisation method), then the sequence of learning rates \\alpha_n should vary little when n is large enough. Therefore, in the search for \\alpha_n, if one always starts from  \\alpha_0, one would waste a lot of time if it turns out that the sequence  \\alpha_n stays far away from \\alpha_0. Instead, one should search for \\alpha_n by starting from \\alpha_{n-1}. The second observation is that \\alpha_n could be larger than \\alpha_{n-1}, and hence one should allow to increase learning rate (and not just decrease as in the section Algorithm). Here is the detailed algorithm for Two-way Backtracking: At step n\n# Set \\gamma _0=\\alpha _{n-1}. Set t = -c\\,m and iteration counter j\\,=\\,0.\n# (Increase learning rate if Armijo's condition is satisfied.) If f(\\mathbf{x})-f(\\mathbf{x}+\\gamma _j\\,\\mathbf{p})\\geq \\gamma_j\\,t,, then while this condition and the condition that \\gamma _j\\leq \\alpha _0 are satisfied, repeatedly set \\gamma_{j+1}=\\gamma_{j}/\\tau  and increase j.\n# (Otherwise, reduce the learning rate if Armijo's condition is not satisfied.) If in contrast f(\\mathbf{x})-f(\\mathbf{x}+\\gamma _0\\,\\mathbf{p}), then until the condition is satisfied that f(\\mathbf{x})-f(\\mathbf{x}+\\gamma_j\\,\\mathbf{p})\\geq \\gamma_j\\,t, repeatedly increment j and set \\gamma_j=\\tau\\,\\gamma_{j-1}\\,.\n# Return \\gamma_j for the learning rate \\alpha _n.\n(In  one can find a description of an algorithm with 1), 3) and 4) above, which was not tested in deep neural networks before the cited paper.)\nOne can save time further by a hybrid mixture between two-way backtracking and the basic standard gradient descent algorithm. This procedure also has good theoretical guarantee and good test performance. Roughly speaking, we run two-way backtracking a few times, then use the learning rate we get from then unchanged, except if the function value increases. Here is precisely how it is done. One choose in advance a number N, and a number m\\leq N.\n# Set iteration counter j=0.\n# At the steps jN+1,\\ldots ,jN+m, use Two-way Backtracking.\n# At each step k in the set jN+m+1,\\ldots ,jN+N-1: Set \\alpha =\\alpha _{k-2}. If f(x_{k-1})-f(x_{k-1}+\\alpha p_{k-1})\\geq 0, then choose \\alpha _{k-1}=\\alpha _{k-2} and x_k=x_{k-1}+\\alpha _{k-1}p_{k-1}. (So, in this case, use the learning rate \\alpha _{k-2} unchanged.) Otherwise, if f(x_{k-1})-f(x_{k-1}+\\alpha p_{k-1}), use Two-way Backtracking. Increase k by 1 and repeat.\n# Increase j by 1.\nTheoretical guarantee (for gradient descent)\nCompared with Wolfe's conditions, which is more complicated, Armijo's condition has a better theoretical guarantee. Indeed, so far backtracking line search and its modifications are the most theoretically guaranteed methods among all numerical optimization algorithms concerning convergence to critical points  and avoidance of saddle points, see below.\nCritical points are points where the gradient of the objective function is 0. Local minima are critical points, but there are critical points which are not local minima. An example is saddle points. Saddle points are critical points, at which there are at least one direction where the function is (local) maximum. Therefore, these points are far from being local minima. For example, if a function has at least one saddle point, then it cannot be convex. The relevance of saddle points to optimisation algorithms is that in large scale (i.e. high-dimensional) optimisation, one likely sees more saddle points than minima, see . Hence, a good optimisation algorithm should be able to avoid saddle points. In the setting of deep learning, saddle points are also prevalent, see . Thus, to apply in deep learning, one needs results for non-convex functions.\nFor convergence to critical points: For example, if the cost function is a real analytic function, then it is shown in  that convergence is guaranteed. The main idea is to use Łojasiewicz inequality which is enjoyed by a real analytic function. For non-smooth functions satisfying Łojasiewicz inequality, the above convergence guarantee is extended, see . In , there is a proof that for every sequence constructed by backtracking line search, a cluster point (i.e. the limit of one subsequence, if the subsequence converges) is a critical point. For the case of a function with at most countably many critical points (such as a Morse function) and compact sublevels, as well as with Lipschitz continuous gradient where one uses standard GD with learning rate f is only assumed to be C^1 and have at most countably many critical points, convergence is guaranteed, see . In the same reference, similarly convergence is guaranteed for other modifications of Backtracking line search (such as Unbounded backtracking gradient descent mentioned in the section \"Upper bound for learning rates\"), and even if the function has uncountably many critical points still one can deduce some non-trivial facts about convergence behaviour.\nIn the stochastic setting, under the same assumption that the gradient is Lipschitz continuous and one uses a more restrictive version (requiring in addition that the sum of learning rates is infinite and the sum of squares of learning rates is finite) of diminishing learning rate scheme (see section \"Stochastic gradient descent\") and moreover the function is strictly convex, then the convergence is established in the well-known result , see  for generalisations to less restrictive versions of a diminishing learning rate scheme. None of these results (for non-convex functions) have been proven for any other optimization algorithm so far.\nFor avoidance of saddle points: For example, if the gradient of the cost function is Lipschitz continuous and one chooses standard GD with learning rate \\mathbf{x}_0 (more precisely, outside a set of Lebesgue measure zero), the sequence constructed will not converge to a non-degenerate saddle point (proven in ), and more generally it is also true that the sequence constructed will not converge to a degenerate saddle point (proven in ). Under the same assumption that the gradient is Lipschitz continuous and one uses a diminishing learning rate scheme (see the section \"Stochastic gradient descent\"), then avoidance of saddle points is established in .\nA special case: (standard) stochastic gradient descent (SGD)\nWhile it is trivial to mention, if the gradient of a cost function is Lipschitz continuous, with Lipschitz constant L, then with choosing learning rate to be constant and of the size 1/L, one has a special case of backtracking line search (for gradient descent). This has been used at least in . This scheme however requires that one needs to have a good estimate for L, otherwise if learning rate is too big (relative to 1/L) then the scheme has no convergence guarantee. One can see what will go wrong if the cost function is a smoothing (near the point 0) of the function f(t)=|t|. Such a good estimate is, however, difficult and laborious in large dimensions. Also, if the gradient of the function is not globally Lipschitz continuous, then this scheme has no convergence guarantee. For example, this is similar to an exercise in , for the cost function f(t)=|t|^{1.5} \\, and for whatever constant learning rate one chooses, with a random initial point the sequence constructed by this special scheme does not converge to the global minimum 0.\nIf one does not care about the condition that learning rate must be bounded by 1/L, then this special scheme has been used much older, at least since 1847 by Cauchy, which can be called standard GD (not to be confused with stochastic gradient descent, which is abbreviated herein as SGD). In the stochastic setting (such as in the mini-batch setting in deep learning), standard GD is called stochastic gradient descent, or SGD.\nEven if the cost function has globally continuous gradient, good estimate of the Lipschitz constant for the cost functions in deep learning may not be feasible or desirable, given the very high dimensions of deep neural networks.  Hence, there is a technique of fine-tuning of learning rates in applying standard GD or SGD. One way is to choose many learning rates from a grid search, with the hope that some of the learning rates can give good results. (However, if the loss function does not have global Lipschitz continuous gradient, then the example with f(t)=|t|^{1.5} \\, above shows that grid search cannot help.) Another way is the so-called adaptive standard GD or SGD, some representatives are Adam, Adadelta, RMSProp and so on, see the article on Stochastic gradient descent. In adaptive standard GD or SGD, learning rates are allowed to vary at each iterate step n, but in a different manner from Backtracking line search for gradient descent. Apparently, it would be more expensive to use Backtracking line search for gradient descent, since one needs to do a loop search until Armijo's condition is satisfied, while for adaptive standard GD or SGD no loop search is needed. Most of these adaptive standard GD or SGD do not have the descent property f(x_{n+1})\\leq f(x_n), for all n, as Backtracking line search for gradient descent. Only a few has this property, and which have good theoretical properties, but they turn out to be special cases of Backtracking line search or more generally Armijo's condition . The first one is when one chooses learning rate to be a constant f(t)=|t| or f(t)=ReLu(t)=\\max\\{t,0\\}.\nSee also\n*Gradient descent\n*Stochastic gradient descent\n*Wolfe conditions\nReferences\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*"
    }
  ]
}