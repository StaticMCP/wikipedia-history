{
  "content": [
    {
      "type": "text",
      "text": "# Spiking neural network\n\n}}\nSpiking neural networks (SNNs) are artificial neural networks (ANN) that mimic natural neural networks. These models leverage timing of discrete spikes as the main information carrier.\nIn addition to neuronal and synaptic state, SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather transmit information only when a membrane potential—an intrinsic quality of the neuron related to its membrane electrical charge—reaches a specific value, called the threshold. When the membrane potential reaches the threshold, the neuron fires, and generates a signal that travels to other neurons which, in turn, increase or decrease their potentials in response to this signal. A neuron model that fires at the moment of threshold crossing is also called a spiking neuron model.\nWhile spike rates can be considered the analogue of the variable output of a traditional ANN, neurobiology research indicated that high speed processing cannot be performed solely through a rate-based scheme. For example humans can perform an image recognition task requiring no more than 10ms of processing time per neuron through the successive layers (going from the retina to the temporal lobe). This time window is too short for rate-based encoding. The precise spike timings in a small set of spiking neurons also has a higher information coding capacity compared with a rate-based approach.\nThe most prominent spiking neuron model is the leaky integrate-and-fire model. In that model, the momentary activation level (modeled as a differential equation) is normally considered to be the neuron's state, with incoming spikes pushing this value higher or lower, until the state eventually either decays or—if the firing threshold is reached—the neuron fires. After firing, the state variable is reset to a lower value.\nVarious decoding methods exist for interpreting the outgoing spike train as a real-value number, relying on either the frequency of spikes (rate-code), the time-to-first-spike after stimulation, or the interval between spikes.\nHistory\nMany multi-layer artificial neural networks are fully connected, receiving input from every neuron in the previous layer and signalling every neuron in the subsequent layer. Although these networks have achieved breakthroughs, they do not match biological networks and do not mimic neurons.\nThe biology-inspired Hodgkin–Huxley model of a spiking neuron was proposed  in 1952. This model described how action potentials are initiated and propagated. Communication between neurons, which requires the exchange of chemical neurotransmitters in the synaptic gap, is described in models such as the integrate-and-fire model, FitzHugh–Nagumo model (1961–1962), and Hindmarsh–Rose model (1984). The leaky integrate-and-fire model (or a derivative) is commonly used as it is easier to compute than Hodgkin–Huxley.\nWhile the notion of an artificial spiking neural network became popular only in the twenty-first century, studies between 1980 and 1995 supported the concept. The first models of this type of ANN appeared to simulate non-algorithmic intelligent information processing systems. However, the notion of the spiking neural network as a mathematical model was first worked on in the early 1970s.\nAs of 2019 SNNs lagged behind ANNs in accuracy, but the gap is decreasing, and has vanished on some tasks.\nUnderpinnings\nInformation in the brain is represented as action potentials (neuron spikes), which may group into spike trains or coordinated waves. A fundamental question of neuroscience is to determine whether neurons communicate by a rate or temporal code. Temporal coding implies that a single spiking neuron can replace hundreds of hidden units on a conventional neural net.\nSNNs define a neuron's current state as its potential (possibly modeled as a differential equation). An input pulse causes the potential to rise and then gradually decline. Encoding schemes can interpret these pulse sequences as a number, considering pulse frequency and pulse interval. Using the precise time of pulse occurrence, a neural network can consider more information and offer better computing properties.\nSNNs compute in the continuous domain. Such neurons test for activation only when their potentials reach a certain value. When a neuron is activated, it produces a signal that is passed to connected neurons, accordingly raising or lowering their potentials.\nThe SNN approach produces a continuous output instead of the binary output of traditional ANNs. Pulse trains are not easily interpretable, hence the need for encoding schemes. However, a pulse train representation may be more suited for processing spatiotemporal data (or real-world sensory data classification). SNNs connect neurons only to nearby neurons so that they process input blocks separately (similar to CNN using filters). They consider time by encoding information as pulse trains so as not to lose information. This avoids the complexity of a recurrent neural network (RNN). Impulse neurons are more powerful computational units than traditional artificial neurons.\nSNNs are theoretically more powerful than so called \"second-generation networks\" defined as ANNs \"based on computational units that apply activation function with a continuous set of possible output values to a weighted sum (or polynomial) of the inputs\"; however, SNN training issues and hardware requirements limit their use. Although unsupervised biologically inspired learning methods are available such as Hebbian learning and STDP, no effective supervised training method is suitable for SNNs that can provide better performance than second-generation networks. Spike-based activation of SNNs is not differentiable, thus gradient descent-based backpropagation (BP) is not available.\nSNNs have much larger computational costs for simulating realistic neural models than traditional ANNs.\nPulse-coupled neural networks (PCNN) are often confused with SNNs. A PCNN can be seen as a kind of SNN.\nResearchers are actively working on various topics. The first concerns differentiability. The expressions for both the forward- and backward-learning methods contain the derivative of the neural activation function which is not differentiable because a neuron's output is either 1 when it spikes, and 0 otherwise. This all-or-nothing behavior disrupts gradients and makes these neurons unsuitable for gradient-based optimization. Approaches to resolving it include:\n* resorting to entirely biologically inspired local learning rules for the hidden units\n* translating conventionally trained \"rate-based\" NNs to SNNs\n* smoothing the network model to be continuously differentiable\n* defining an SG (Surrogate Gradient) as a continuous relaxation of the real gradients\nThe second concerns the optimization algorithm. Standard BP can be expensive in terms of computation, memory, and communication and may be poorly suited to the hardware that implements it (e.g., a computer, brain, or neuromorphic device).\nIncorporating additional neuron dynamics such as Spike Frequency Adaptation (SFA) is a notable advance, enhancing efficiency and computational power. These neurons sit between biological complexity and computational complexity. Originating from biological insights, SFA offers significant computational benefits by reducing power usage, especially in cases of repetitive or intense stimuli. This adaptation improves signal/noise clarity and introduces an elementary short-term memory at the neuron level, which in turn, improves accuracy and efficiency. This was mostly achieved using compartmental neuron models. The simpler versions are of neuron models with adaptive thresholds, are an indirect way of achieving SFA. It equips SNNs with improved learning capabilities, even with constrained synaptic plasticity, and elevates computational efficiency. This feature lessens the demand on network layers by decreasing the need for spike processing, thus lowering computational load and memory access time—essential aspects of neural computation. Moreover, SNNs utilizing neurons capable of SFA achieve levels of accuracy that rival those of conventional ANNs, while also requiring fewer neurons for comparable tasks. This efficiency streamlines the computational workflow and conserves space and energy, while maintaining technical integrity. High-performance deep spiking neural networks can operate with 0.3 spikes per neuron.\nApplications\nSNNs can in principle be applied to the same applications as traditional ANNs. In addition, SNNs can model the central nervous system of biological organisms, such as an insect seeking food without prior knowledge of the environment. Due to their relative realism, they can be used to study biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function, recordings of this circuit can be compared to the output of a corresponding SNN, evaluating the plausibility of the hypothesis. SNNs lack effective training mechanisms, which can complicate some applications, including computer vision.\nWhen using SNNs for image based data, the images need to be converted into binary spike trains. Types of encodings include:\n* Temporal coding; generating one spike per neuron, in which spike latency is inversely proportional to the pixel intensity.\n* Rate coding: converting pixel intensity into a spike train, where the number of spikes is proportional to the pixel intensity.\n* Direct coding; using a trainable layer to generate a floating-point value for each time step. The layer converts each pixel at a certain time step into a floating-point value, and then a threshold is used on the generated floating-point values to pick either zero or one.\n* Phase coding; encoding temporal information into spike patterns based on a global oscillator.\n* Burst coding; transmitting spikes in bursts, increasing communication reliability.\nSoftware\nA diverse range of application software can simulate SNNs. This software can be classified according to its uses:\nSNN simulation\nThese simulate complex neural models. Large networks usually require lengthy processing. Candidates include:\n* Brian – developed by Romain Brette and Dan Goodman at the École Normale Supérieure;\n* GENESIS (the GEneral NEural SImulation System) – developed in James Bower's laboratory at Caltech;\n* NEST – developed by the NEST Initiative;\n* NEURON – mainly developed by Michael Hines, John W. Moore and Ted Carnevale in Yale University and Duke University;\n* RAVSim (Runtime Tool)  – mainly developed by Sanaullah in Bielefeld University of Applied Sciences and Arts;\n* [https://snntorch.readthedocs.io/en/latest/ snnTorch] – an open-source Python library that simplifies building spiking neural networks and implementing gradient-based training using PyTorch.\nHardware\nEfforts to implement hardware-based spiking neural networks (SNNs) began in the 1980s, when researchers began exploring brain-inspired neuromorphic systems. In the following decades, advancements in semiconductor technologies enabled the development of several notable projects.\nOne such project is [https://apt.cs.manchester.ac.uk/projects/SpiNNaker/ SpiNNaker], developed at the University of Manchester, which utilizes millions of processing cores for large-scale simulation of spiking neurons.\n[https://www.research.ibm.com/artificial-intelligence/hardware/brain-inspired-technology/ TrueNorth], developed by IBM, is one of the first commercial neuromorphic chips, designed for energy-efficient and parallel processing.\n[https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html Loihi], an Intel research chip, focuses on online learning and adaptability in neuromorphic modeling.\nIn research contexts, platforms such as [https://brainscales.kip.uni-heidelberg.de/ BrainScaleS], developed in Europe, integrate analog and digital circuitry to accelerate neural simulations. [https://web.stanford.edu/group/brainsinsilicon/neurogrid.html Neurogrid], from Stanford University, was designed to efficiently simulate biological neurons and synapses. Dynap-se  models developed by iniLabs, are a family of low-power, event-based neuromorphic chip intended for use in robotics and Internet of Things (IoT) applications.\nIn addition, hardware based on memristors and other emerging memory technologies is being explored for the implementation of SNNs, with the goal of achieving lower power consumption and improved compatibility with biological neural models.\nSutton and Barto proposed that future neuromorphic architectures will comprise billions of nanosynapses, which require a clear understanding of the accompanying physical mechanisms. Experimental systems based on ferroelectric tunnel junctions have been used to show that STDP can be harnessed from heterogeneous polarization switching. Through combined scanning probe imaging, electrical transport and atomic-scale molecular dynamics, conductance variations can be modelled by nucleation-dominated domain reversal. Simulations showed that arrays of ferroelectric nanosynapses can autonomously learn to recognize patterns in a predictable way, opening the path towards unsupervised learning.\nBenchmarks\nClassification capabilities of spiking networks trained according to unsupervised learning methods have been tested on benchmark datasets such as Iris, Wisconsin Breast Cancer or Statlog Landsat dataset. Various approaches to information encoding and network design have been used such as a 2-layer feedforward network for data clustering and classification. Based on Hopfield (1995) the authors implemented models of local receptive fields combining the properties of radial basis functions and spiking neurons to convert input signals having a floating-point representation into a spiking representation.\nSee also\n*CoDi\n*Cognitive architecture\n*Cognitive map\n*Cognitive computer\n*Computational neuroscience\n*Neural coding\n*Neural correlate\n*Neural decoding\n*Neuroethology\n*Neuroinformatics\n*Models of neural computation\n*Motion perception\n*Systems neuroscience\nReferences"
    }
  ]
}