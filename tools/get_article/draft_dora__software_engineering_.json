{
  "content": [
    {
      "type": "text",
      "text": "# Draft:DORA (software engineering)\n\n----\nDORA metrics are a set of four key performance indicators used to assess software delivery performance in DevOps environments. They were identified by Nicole Forsgren, Jez Humble, and Gene Kim through multi-year industry surveys and published in the Accelerate framework. The four metrics are Deployment Frequency, Lead Time for Changes, Change Failure Rate, and Time to Restore Service (or Mean Time to Recover). These metrics capture both throughput (speed of delivery) and stability (reliability of deployments). Research shows that organizations scoring highly on these metrics tend to have markedly better outcomes: elite performers deploy changes about 46× more often and recover from failures about 2,604× faster than low performers. Google Cloud states that performance on these metrics predicts overall business performance and engineering team well-being. In practice, teams use DORA metrics to set improvement goals and track progress over time. They have become an industry-standard baseline for DevOps maturity and continuous improvement.\nHistory and background\nThe DORA metrics originated in the early 2010s from systematic research on DevOps performance. A 2014 study surveyed over 2,000 organizations (23,000+ responses) to understand what makes technology teams high-performing. Forsgren, Humble, and Kim identified a “true constant” set of four metrics measuring software delivery performance. These findings were presented in annual State of DevOps reports and later formalized in the 2018 book Accelerate. Google’s DevOps Research and Assessment (DORA) team – founded by Forsgren, Humble, and Kim – continued the work, publishing annual reports and tools. DORA is now part of Google Cloud, led by Nathen Harvey and Derek DeBellis. Surveys by Puppet (2015–2020) and Google Cloud/DORA (2019–present) reinforced these metrics as core indicators. Over time, the metrics and survey methods were refined; for example, in 2023 the change-failure rate and recovery-time questions were updated. By 2024, reports explicitly noted that these metrics had become “the industry standard for measuring software delivery performance”.\nDescription and definition\nThe four DORA metrics are grouped into throughput (speed) and stability (reliability) measures.\n* Change Lead Time: The elapsed time from code commit to deployment in production.\n* Deployment Frequency: How often code is deployed.\n* Change Failure Rate: The percentage of deployments causing production failure.\n* Time to Restore Service: Average time to recover from a failed deployment.\nThroughput metrics (Deployment Frequency, Lead Time) assess speed, while stability metrics (Change Failure Rate, Time to Restore) assess reliability. They are leading indicators for delivery performance. Best practice is to track them per application or service over time. Atlassian notes they work best when considered together.\nSignificance and impact\nHigh performance on DORA metrics correlates with faster time-to-market, better quality, and higher developer satisfaction. Elite teams deploy 46× more often and recover 2,604× faster than low performers. Experts consider them “important benchmarks” for engineering organizations. In practice, they help identify bottlenecks and track improvements. Academic studies support their value for continuous improvement.\nKey figures and organizations\n* DevOps Research & Assessment (DORA): Founded by Forsgren, Humble, and Kim.\n* Nicole Forsgren, Jez Humble, Gene Kim: Original researchers.\n* Puppet: Published State of DevOps reports reinforcing DORA metrics.\n* Google Cloud: Current steward of DORA research.\n* Atlassian: Provides guidance on using DORA metrics.\n* GitLab: Includes DORA metrics in its Global DevSecOps survey.\nCurrent status and recent developments\nRecent reports added dimensions such as system reliability and developer experience. Psychological safety is a strong predictor of success on all four metrics. Researchers have applied DORA metrics in contexts like mobile development.\nControversies and criticisms\nThe DORA team warns against using the metrics as rigid targets due to Goodhart's law. They should be interpreted with context, not in isolation. Critics note they focus on delivery outcomes but not all aspects of software value or sustainability.\nSee also\n* DevOps\n* Continuous integration\n* Continuous delivery\n* Agile software development\n* Capability Maturity Model Integration\n* Lean software development\nReferences"
    }
  ]
}