{
  "content": [
    {
      "type": "text",
      "text": "# Software composition analysis__disambig_1\n\nSoftware composition analysis (SCA) is a practice in the fields of Information technology and software engineering for analyzing custom-built software applications to detect embedded open-source software and detect if they are up-to-date, contain security flaws, or have licensing requirements.\nBackground\nIt is a common software engineering practice to develop software by using different components. Using software components segments the complexity of larger elements into smaller pieces of code and increases flexibility by enabling easier reuse of components to address new requirements. The practice has widely expanded since the late 1990s with the popularization of open-source software (OSS) to help speed up the software development process and reduce time to market.\nHowever, using open-source software introduces many risks for the software applications being developed. These risks can be organized into 5 categories:\n* OSS Version Control: risks of changes introduced by new versions\n* Security: risks of vulnerabilities in components - Common Vulnerabilities & Exposures (or CVEs)\n* License: risks of Intellectual property (IP) legal requirements\n* Development: risks of compatibility between existing codebase and open-source software\n* Support: risk of poor documentation and Obsolete software components\nShortly after the foundation of the Open Source Initiative in February 1998, the risks associated with OSS were raised and organizations tried to manage this using spreadsheets and documents to track all the open source components used by their developers.\nFor organizations using open-source components extensively, there was a need to help automate the analysis and management of open source risk. This resulted in a new category of software products called Software Composition Analysis (SCA) which helps organizations manage open source risk.\nSCA strives to detect all the 3rd party components in use within a software application to help reduce risks associated with security vulnerabilities, IP licensing requirements, and obsolescence of components being used.\nPrinciple of operation\nSCA products typically work as follows:\n* An engine scans the software source code, and the associated artifacts used to compile a software application.\n* The engine identifies the OSS components and their versions and usually stores this information in a database creating a catalog of OSS in use in the scanned application.\n* This catalog is then compared to databases referencing known security vulnerabilities for each component, the licensing requirements for using the component, and the historical versions of the component. For security vulnerability detection, this comparison is typically made against known security vulnerabilities (CVEs) that are tracked in the National Vulnerability Database (NVD). Some products use an additional proprietary database of vulnerabilities. For IP / Legal Compliance, SCA products will extract and evaluate the type of licensing used for the OSS component. Versions of components are extracted from popular open source repositories such as GitHub, Maven, PyPi, NuGet, and many others.\n* Modern SCA systems have incorporated advanced analysis techniques to improve accuracy and reduce false positives. Notable contributions include vulnerable method analysis, which determines whether vulnerable methods identified in dependencies are actually reachable from the application code. This approach, pioneered by Asankhaya Sharma and colleagues, uses call graph analysis to trace execution paths from application entry points to vulnerability-specific sinks in third-party libraries.\n* Hybrid static-dynamic analysis techniques combine statically-constructed call graphs with dynamic instrumentation to improve the performance of false positive elimination. This modular approach addresses limitations of purely static analysis, which can introduce both false positives and false negatives on real-world projects.\n* Machine learning-based vulnerability curation automates the process of building and maintaining vulnerability databases by predicting the vulnerability-relatedness of data items from various sources such as bug tracking systems, commits, and mailing lists. These systems use self-training techniques to iteratively improve model quality and include deployment stability metrics to evaluate new models before production deployment.\n* Natural language processing techniques for automated vulnerability identification analyze commit messages and bug reports to identify security-related issues that may not have been publicly disclosed. This approach uses machine learning classifiers trained on textual features extracted from development artifacts to discover previously unknown vulnerabilities in open-source libraries.\n* The results are then made available to end users using different digital formats. The content and format depend on the SCA product and may include guidance to evaluate and interpret the risk, and recommendations especially when it concerns the legal requirements of open source components such as strong or weak copyleft licensing. The output may also contain a Software Bill of Materials (SBOM) detailing all the open source components and associated attributes used in a software application\nAdvanced techniques\nSince the early 2010s, researchers have developed several advanced techniques to improve the accuracy and efficiency of SCA tools:\nVulnerable method analysis\nVulnerable method analysis addresses the problem of determining whether a vulnerability in a third-party library poses an actual risk to an application. Rather than simply detecting the presence of vulnerable libraries, this technique analyzes whether the specific vulnerable methods within those libraries are reachable from the application's execution paths. The method was invented and first implemented at SourceClear under the leadership of Asankhaya Sharma between 2015 and 2017. The approach involves constructing call graphs that map the relationships between application code and library methods, then determining if there exists a path from application entry points to vulnerability-specific sinks in the libraries.\nMachine learning for vulnerability databases\nTraditional vulnerability databases rely on manual curation by security researchers, which can be time-intensive and may miss relevant vulnerabilities. Machine learning approaches automate this process by training models to predict whether data items from various sources (such as bug reports, commits, and mailing lists) are vulnerability-related. These systems implement complete pipelines from data collection through model training and prediction, with iterative improvement mechanisms that generate better models as new data becomes available.\nStatic analysis for library compatibility\nAs SCA tools increasingly recommend library updates to address vulnerabilities, ensuring compatibility becomes critical. Advanced static analysis techniques can automatically detect API incompatibilities that would be introduced by library upgrades, enabling automated vulnerability remediation without breaking existing functionality. These lightweight analyses are designed to integrate into continuous integration and continuous delivery pipelines.\nUsage\nAs SCA impacts different functions in organizations, different teams may use the data depending on the organization's corporation size and structure. The IT department will often use SCA for implementing and operationalizing the technology with common stakeholders including the chief information officer (CIO), the Chief Technology Officer (CTO), and the Chief Enterprise Architects (EA). Security and license data are often used by roles such as Chief Information Security Officers (CISO) for security risks, and Chief IP / Compliance officer for Intellectual Property risk management.\nDepending on the SCA product capabilities, it can be implemented directly within a developer's Integrated Development Environment (IDE) who uses and integrates OSS components, or it can be implemented as a dedicated step in the software quality control process.\nSCA products, and particularly their capacity to generate an SBOM is required in some countries such as the United States to enforce the security of software delivered to one of their agencies by a vendor.\nAnother common use case for SCA is for Technology Due diligence. Prior to a Merger & Acquisition (M&A) transaction, Advisory firms review the risks associated with the software of the target firm.\nStrengths\nThe automatic nature of SCA products is their primary strength. Developers don't have to manually do an extra work when using and integrating OSS components. The automation also applies to indirect references to other OSS components within code and artifacts.\nModern SCA implementations have significantly improved accuracy through advanced analysis techniques. Vulnerable method analysis reduces false positives by determining actual reachability of vulnerable code paths, while machine learning approaches for vulnerability curation help maintain more comprehensive and up-to-date vulnerability databases. These advances address many traditional limitations of metadata-only approaches.\nWeaknesses\nConversely, some key weaknesses of current SCA products may include:\n* Complex and labor-intensive deployment that can take months to get fully operational\n* Each product uses its own proprietary database of OSS components that can vary dramatically in terms of size and coverage\n* Limiting vulnerability data to reporting only on vulnerabilities officially reported in the NVD (which can be months after the vulnerability was originally discovered)\n* Lack of automated guidance on actions to take based on SCA reports and data\n* Lack of guidance on the legal requirements of OSS licenses that are detected\nSee also\n* Security testing\n* Open-source software\n* Common Vulnerabilities and Exposures\n* Open-source license\n* Software intelligence\n* Asankhaya Sharma\n* Static program analysis\n* Call graph\nReferences"
    }
  ]
}