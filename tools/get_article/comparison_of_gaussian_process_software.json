{
  "content": [
    {
      "type": "text",
      "text": "# Comparison of Gaussian process software\n\nThis is a comparison of statistical analysis software that allows doing inference with Gaussian processes often using approximations.\nThis article is written from the point of view of Bayesian statistics, which may use a terminology different from the one commonly used in kriging. The next section should clarify the mathematical/computational meaning of the information provided in the table independently of contextual terminology.\nDescription of columns\nThis section details the meaning of the columns in the table below.\nSolvers\nThese columns are about the algorithms used to solve the linear system defined by the prior covariance matrix, i.e., the matrix built by evaluating the kernel.\n* Exact: whether generic exact algorithms are implemented. These algorithms are usually appropriate only up to some thousands of datapoints.\n* Specialized: whether specialized exact algorithms for specific classes of problems are implemented. Supported specialized algorithms may be indicated as:\n** Kronecker: algorithms for separable kernels on grid data.\n** Toeplitz: algorithms for stationary kernels on uniformly spaced data.\n** Semisep.: algorithms for semiseparable covariance matrices.\n** Sparse: algorithms optimized for sparse covariance matrices.\n** Block: algorithms optimized for block diagonal covariance matrices.\n** Markov: algorithms for kernels which represent (or can be formulated as) a Markov process.\n* Approximate: whether generic or specialized approximate algorithms are implemented. Supported approximate algorithms may be indicated as:\n** Sparse: algorithms based on choosing a set of \"inducing points\" in input space, or  more in general imposing a sparse structure on the inverse of the covariance matrix.\n** Hierarchical: algorithms which approximate the covariance matrix with a hierarchical matrix.\nInput\nThese columns are about the points on which the Gaussian process is evaluated, i.e. x if the process is f(x).\n* ND: whether multidimensional input is supported. If it is, multidimensional output is always possible by adding a dimension to the input, even without direct support.\n* Non-real: whether arbitrary non-real input is supported (for example, text or complex numbers).\nOutput\nThese columns are about the values yielded by the process, and how they are connected to the data used in the fit.\n* Likelihood: whether arbitrary non-Gaussian likelihoods are supported.\n* Errors: whether arbitrary non-uniform correlated errors on datapoints are supported for the Gaussian likelihood. Errors may be handled manually by adding a kernel component, this column is about the possibility of manipulating them separately. Partial error support may be indicated as:\n** iid: the datapoints must be independent and identically distributed.\n** Uncorrelated: the datapoints must be independent, but can have different distributions.\n** Stationary: the datapoints can be correlated, but the covariance matrix must be a Toeplitz matrix, in particular this implies that the variances must be uniform.\nHyperparameters\nThese columns are about finding values of variables which enter somehow in the definition of the specific problem but that can not be inferred by the Gaussian process fit, for example parameters in the formula of the kernel.\n* Prior: whether specifying arbitrary hyperpriors on the hyperparameters is supported.\n* Posterior: whether estimating the posterior is supported beyond point estimation, possibly in conjunction with other software.\nIf both the \"Prior\" and \"Posterior\" cells contain \"Manually\", the software provides an interface for computing the marginal likelihood and its gradient w.r.t. hyperparameters, which can be feed into an optimization/sampling algorithm, e.g., gradient descent or Markov chain Monte Carlo.\nLinear transformations\nThese columns are about the possibility of fitting datapoints simultaneously to a process and to linear transformations of it.\n* Deriv.: whether it is possible to take an arbitrary number of derivatives up to the maximum allowed by the smoothness of the kernel, for any differentiable kernel. Example partial specifications may be the maximum derivability or implementation only for some kernels. Integrals can be obtained indirectly from derivatives.\n* Finite: whether finite arbitrary \\mathbb R^n \\to \\mathbb R^m linear transformations are allowed on the specified datapoints.\n* Sum: whether it is possible to sum various kernels and access separately the processes corresponding to each addend. It is a particular case of finite linear transformation but it is listed separately because it is a common feature.\nComparison table\n{| class=\"wikitable sortable sort-under\" style=\"font-size: 90%; text-align: center; width: auto;\"\n|-\n! rowspan=\"2\" | Name\n! rowspan=\"2\" | License\n! rowspan=\"2\" | Language\n! colspan=\"3\" | Solvers\n! colspan=\"2\" | Input\n! colspan=\"2\" | Output\n! colspan=\"2\" | Hyperparameters\n! colspan=\"3\" | Linear transformations\n! rowspan=\"2\" | Name\n|-\n! Exact\n!\n!\n! ND\n!\n! Likelihood\n! Errors\n! Prior\n! Posterior\n!\n! Finite\n! Sum\n|-\n! PyMC\n|\n| Python\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! PyMC\n|-\n! Stan\n|\n| custom\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! Stan\n|-\n! scikit-learn\n|\n| Python\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! scikit-learn\n|-\n! [http://www.cs.toronto.edu/%7Eradford/fbm.software.html fbm]\n|\n| C\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [http://www.cs.toronto.edu/%7Eradford/fbm.software.html fbm]\n|-\n! [http://www.gaussianprocess.org/gpml/code/matlab/doc/index.html GPML]\n|\n| MATLAB\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [http://www.gaussianprocess.org/gpml/code/matlab/doc/index.html GPML]\n|-\n! [https://research.cs.aalto.fi/pml/software/gpstuff/ GPstuff]\n|\n| MATLAB, R\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://research.cs.aalto.fi/pml/software/gpstuff/ GPstuff]\n|-\n! [https://sheffieldml.github.io/GPy/ GPy]\n|\n| Python\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://sheffieldml.github.io/GPy/ GPy]\n|-\n! [https://www.gpflow.org GPflow]\n|\n| Python\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://www.gpflow.org GPflow]\n|-\n! [https://gpytorch.ai GPyTorch]\n|\n| Python\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://gpytorch.ai GPyTorch]\n|-\n! [https://CRAN.R-project.org/package=GPvecchia GPvecchia]\n|\n| R\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://CRAN.R-project.org/package=GPvecchia GPvecchia]\n|-\n! [https://github.com/marionmari/pyGPs pyGPs]\n|\n| Python\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://github.com/marionmari/pyGPs pyGPs]\n|-\n! [https://CRAN.R-project.org/package=gptk gptk]\n|\n| R\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://CRAN.R-project.org/package=gptk gptk]\n|-\n! [https://celerite.readthedocs.io/en/stable/ celerite]\n|\n| Python, Julia, C++\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://celerite.readthedocs.io/en/stable/ celerite]\n|-\n! [http://george.readthedocs.io george]\n|\n| Python, C++\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [http://george.readthedocs.io george]\n|-\n! [https://github.com/google/neural-tangents neural-tangents]\n|\n| Python\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://github.com/google/neural-tangents neural-tangents]\n|-\n! [https://cran.r-project.org/package=DiceKriging DiceKriging]\n|\n| R\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://cran.r-project.org/package=DiceKriging DiceKriging]\n|-\n! [https://openturns.github.io/www/ OpenTURNS]\n|\n| Python, C++\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://openturns.github.io/www/ OpenTURNS]\n|-\n! [http://www.uqlab.com/ UQLab]\n|\n| MATLAB\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [http://www.uqlab.com/ UQLab]\n|-\n! [https://sumo.ilabt.imec.be/home/software/oodace ooDACE]\n|\n| MATLAB\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://sumo.ilabt.imec.be/home/software/oodace ooDACE]\n|-\n! [http://www.omicron.dk/dace.html DACE]\n|\n| MATLAB\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [http://www.omicron.dk/dace.html DACE]\n|-\n! [https://CRAN.R-project.org/package=GpGp GpGp]\n|\n| R\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://CRAN.R-project.org/package=GpGp GpGp]\n|-\n! [https://CRAN.R-project.org/package=SuperGauss SuperGauss]\n|\n| R, C++\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://CRAN.R-project.org/package=SuperGauss SuperGauss]\n|-\n! [https://sourceforge.net/projects/kriging/ STK]\n|\n| MATLAB\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://sourceforge.net/projects/kriging/ STK]\n|-\n! [https://github.com/GeoStat-Framework/GSTools GSTools]\n|\n| Python\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://github.com/GeoStat-Framework/GSTools GSTools]\n|-\n! [https://github.com/GeoStat-Framework/PyKrige PyKrige]\n|\n| Python\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://github.com/GeoStat-Framework/PyKrige PyKrige]\n|-\n! [https://github.com/ChristophJud/GPR GPR]\n|\n| C++\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://github.com/ChristophJud/GPR GPR]\n|-\n! [https://celerite2.readthedocs.io/en/latest/ celerite2]\n|\n| Python\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://celerite2.readthedocs.io/en/latest/ celerite2]\n|-\n! [https://smt.readthedocs.io/en/latest/ SMT]\n|\n| Python\n|\n|\n| , other}}\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://smt.readthedocs.io/en/latest/ SMT]\n|-\n! [https://gpjax.readthedocs.io/en/latest/ GPJax]\n|\n| Python\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://gpjax.readthedocs.io/en/latest/ GPJax]\n|-\n! [https://github.com/wesselb/stheno Stheno]\n|\n| Python\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n! [https://github.com/wesselb/stheno Stheno]\n|-\n! [https://docs.rs/egobox-gp/latest/egobox_gp/ Egobox-gp]\n}}\nExternal links\n* [http://www.gaussianprocess.org] The website hosting C.&nbsp;E.&nbsp;Rasmussen's book Gaussian processes for machine learning; contains a (partially outdated) list of software."
    }
  ]
}