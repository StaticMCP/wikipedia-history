{
  "content": [
    {
      "type": "text",
      "text": "# Garak (software)\n\n| latest release version = 0.13.0\n| latest release date =\n| repo =\n| programming language = Python\n| operating system = Cross-platform\n| genre = Security\n| license = Framework: Apache License\n| website =\n}}\ngarak is a computer security tool that provides information about Â LLM security vulnerabilities and aids in penetration testing and red teaming of language models and dialog systems. It is supported by Nvidia. Officially the name is short for \"generative AI red-teaming & assessment kit\".\ngarak is described as the leading LLM vulnerability scanner in an independent 2024 review by Fujitsu Research. It is used and recommended as tooling in articles from Microsoft, Trend Micro, NVIDIA and Cisco, and has been covered in major IT news outlets.\nHistory\ngarak was developed in Spring 2023 by Prof. Leon Derczynski of ITU Copenhagen during a sabbatical at the University of Washington. It was first released under GPL on 13 June 2023. The license was later updated to Apache 2.0. The software is now homed at NVIDIA, where it lives as an open-source project with long-term support, and has been available via the NVIDIA public GitHub since November 2024.\nFramework\nThe main components in garak are probes, generators, and detectors. Probes manage attacks and implement an adversarial technique. Generators abstract away targets, which may be an LLM, a dialogue system, or anything that can take text and return text (plus optionally other modalities). Probes attempt to attack generators and pass the resulting output to a detector. The detectors assess whether or not the output indicates a successful attack. The whole is compiled into reporting by an HTML page and a JSON object summarizing results.\nSee also\n*Nmap\n*Metasploit Framework\nReferences\nExternal links\n*\n* [https://pypi.org/project/garak/ garak on the Python Package Index]\n*\n* [https://infocondb.org/con/def-con/def-con-32/garak garak talk at DEF CON 2024]\n* [https://docs.nvidia.com/nemo/guardrails/latest/evaluation/llm-vulnerability-scanning.html Nvidia NeMo Guardrails - LLM Vulnerability]\n* [https://build.nvidia.com/nvidia/safety-for-agentic-ai Nvidia Safety for Agentic AI]"
    }
  ]
}