{
  "content": [
    {
      "type": "text",
      "text": "# Draft:Real-time Adversarial Intelligence and Decision-making\n\n----\nReal-time Adversarial Intelligence and Decision-making (RAID) was a DARPA research program, and the eponymous prototype software system. The program developed computational methods to infer the intent and future actions of an adversary. It did so based on partial and potentially deceptive observations about the prior events on the battlefield.\nThe RAID program produced a technology called LG-RAID that found applications in the United States Army, United States Marine Corps, and other military organizations of the United States\nHistory\nA history of DARPA contributions to knowledge representation and reasoning depicts RAID as one of the DARPA programs in Cognitive Systems area of the 2000s, along with others such as the JAGUAR program\nRelated programs of the same period, intended to develop tools for automation of military decision-making, were Deep Green and CADET\nThe DARPA Real-time Adversarial Intelligence and Decision-making (RAID) program started in 2004 and ended in 2008.\nAfter the RAID program ended, an output of the program called the LG-RAID technology continued its development in follow-on projects. By 2021, the United States Army, Navy, Air Force, DARPA and Missile Defense Agency funded a total of 18 projects that researched the use of LG-RAID technology in planning, wargaming, predicting enemy actions, and estimating results of a military operation.\nOperation\nRAID was intended to be used by the staff or commander and staff of the United States Army unit such as a reinforced company, battalion, or Brigade combat team, during the execution of a military operation. The program focused on tactical combat of infantry (supported by armor and air platforms) against a guerrilla-like enemy force in an urban terrain.\nRAID software resided on a laptop computer. Using the computer, the Blue unit's commander or the staff officers entered the input to RAID, or alternatively this input arrived to RAID from upstream computer systems. Large fraction of this input dynamically changes and arrives at unpredictable moments as the combat mission is being executed. The input consisted of:\n* the Blue force composition;\n* the Blue mission plan;\n* 3D map of the urban area;\n* known concentrations of noncombatants such as markets;\n* culturally sensitive areas such as worship houses;\n* continuous updates on the locations and status of the Blue force;\n* Blue forces' reports (electronic, verbal or textual) regarding the observed positions and strength of the * Red force, or fires received from the Red force.\nTaking this input, RAID's algorithms automatically produced the following outputs:\n* estimated actual locations and strength of the Red force (note that these are normally concealed and are not observed by the Blue force);\n* the current intent of the Red force;\n* potential deceptions that the Red force may be performing;\n* the Red force's future locations (as a function of time),\n* anticipated (as a function of time) future Red fires the Blue force.\n* recommendations (recommended Course of Action) to the Blue force on how to prevent or to parry the anticipated actions of the Red force.\nTechnologies\nThe RAID program explored several algorithmic approaches. One group of algorithms estimated the \"human\" aspects of battlefield behaviors with a cognitive model, using a Bayesian belief network.\nAnother group of algorithms estimated probable Red deceptions.\nAnother algorithm performed fast heuristic game solving.\nThis algorithm eventually led to a technology called LG-RAID.\nEvaluation\nThe RAID program performed a number of evaluation experiments. Some of the series of experiments consisted of multiple wargames executed by live (human) Red and Blue commanders, but in a simulated computer wargaming environment called OneSAF In half of wargames, the Blue commander received the support of a human team of competent assistants (staff) whose responsibilities included producing estimates of the Red locations and intended future actions. These wargames constituted the control group. In the other half of wargames, Blue commander operated without a human staff. Instead, he obtained a similar support from the RAID tool. These wargames constituted the test group.\nIn these series of experiments, RAID generally outperformed humans. RAID was more accurate in estimating the current and future locations of Red forces. When a commander used RAID's suggestions, he won a higher percentage of battles than when he was assisted by human staff.\nRAID system was also used in realistic military exercises via the interface of the system called FBCB2 Force XXI Battle Command Brigade and Below.\nAnother series of evaluations focused on the suitability of the tool to the cognitive capabilities of the users; it identified several important requirements for improvements of the user interfaces\nLegacy\nDuring and after the RAID program, military organizations of the United States (Army, Navy, Air Force, DARPA and Missile Defense Agency) initiated a number of programs (a total of 18 by 2021) that used a technology developed in the RAID program, called LG-RAID, in planning, wargaming, predicting enemy actions, and estimating results of a military operation. The technology was also integrated with several commercial and government-owned systems used for military analysis, wargaming and decision-making.\nCriticisms\nThe RAID program was questioned because \"...machine intelligence may not be the perfect match for the realm of war for the very reason that it remains a human realm, even with machines fighting in it,\" and because it may tempt the commander to micromanage subordinates.\nA concern was expressed about the possibility of using technologies like RAID in decisions pertaining to nuclear conflicts, where artificial intelligence might mislead a decision-maker into an incorrect assessment of risks.\nSimilarly, is was hypothesized that AI-enabled tools like RAID will be destabilizing if the humans will trust the AI as a panacea for the cognitive fallibility of human analysis.\n:Category:Artificial intelligence engineering\n:Category:Automated planning and scheduling\n:Category:Military intelligence\n:Category:Command and control\n:Category:Military exercises and wargames\n:Category:Military plans\nReferences"
    }
  ]
}