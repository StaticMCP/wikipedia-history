{
  "content": [
    {
      "type": "text",
      "text": "# Binary independence model__disambig_1\n\nIn computing and information science, the binary independence model (BIM)  is a probabilistic information retrieval technique. The model makes some simple assumptions to make the estimation of document/query similarity probable and feasible.\nDefinitions\nThe binary independence assumption states that documents are binary vectors; that is, only the presence or absence of terms in documents is recorded. Terms are independently distributed in the set of relevant documents and they are also independently distributed in the set of irrelevant documents.\nThe representation is an ordered set of Boolean variables. That is, the representation of a document or query is a vector with one Boolean element for each term under consideration. More specifically, a document is represented by a vector  where  if term t is present in the document d and  if it's not. Many documents can have the same vector representation with this simplification. Queries are represented in a similar way.\n\"Independence\" signifies that terms in the document are considered independently from each other and  no association between terms is modeled. This assumption is very limiting, but it has been shown that it gives good enough results for many situations. This independence is the \"naive\" assumption of a Naive Bayes classifier, where properties that imply each other are nonetheless treated as independent for the sake of simplicity. This assumption allows the representation to be treated as an instance of a Vector space model by considering each term as a value of 0 or 1 along a dimension orthogonal to the dimensions used for the other terms.\nThe probability P(R|d,q) that a document is relevant derives from the probability of relevance of the terms vector of that document P(R|x,q). By using the Bayes rule we get:\n: P(R|x,q) = \\frac{P(x|R,q)*P(R|q)}{P(x|q)}\nwhere P(x|R=1,q) and P(x|R=0,q) are the probabilities of retrieving a relevant or nonrelevant document, respectively. If so, then that document's representation is x.\nThe exact probabilities can not be known beforehand, so estimates from statistics about the collection of documents must be used.\nP(R=1|q) and P(R=0|q) indicate the previous probability of retrieving a relevant or nonrelevant document respectively for a query q. If, for instance, we knew the percentage of relevant documents in the collection, then we could use it to estimate these probabilities.\nSince a document is either relevant or nonrelevant to a query we have that:\n: P(R=1|x,q) + P(R=0|x,q) = 1\nQuery Terms Weighting\nGiven a binary query and the dot product as the similarity function between a document and a query, the problem is to assign weights to the\nterms in the query such that the retrieval effectiveness will be high. Let p_i and q_i be the probability that a relevant document and an irrelevant document has the  term respectively. Yu and Salton, who first introduce BIM, propose that the weight of the  term is an increasing function of Y_i =  \\frac{p_i *(1-q_i)}{(1-p_i)*q_i}. Thus, if Y_i is higher than Y_j, the weight\nof term  will be higher than that of term . Yu and Salton showed that such a weight assignment to query terms yields better retrieval effectiveness than if query terms are equally weighted. Robertson and Spärck Jones later showed that if the  term is assigned the weight of \\log Y_i, then optimal retrieval effectiveness is obtained under the binary independence assumption.\nThe binary independence model was introduced by Yu and Salton. The term binary independence model was coined by Robertson and Spärck Jones who used the log-odds probability of the probabilistic relevance model to derive \\log Y_i where the log-odds probability is shown to be rank equivalent to the probability of relevance (i.e., P(R|d,q)) by Luk, obeying the probability ranking principle.\n}}"
    }
  ]
}