{
  "content": [
    {
      "type": "text",
      "text": "# Blocking (computing)\n\nIn computing, a process that is blocked is waiting for some event, such as a resource becoming available or the completion of an I/O operation.\nOnce the event occurs for which the process is waiting (\"is blocked on\"), the process is advanced from blocked state to an imminent one, such as runnable.\nIn a multitasking computer system, individual tasks, or threads of execution, must share the resources of the system. Shared resources include: the CPU, network and network interfaces, memory and disk.\nWhen one task is using a resource, it is generally not possible, or desirable, for another task to access it. The techniques of mutual exclusion are used to prevent this concurrent use. When the other task is blocked, it is unable to execute until the first task has finished using the shared resource.\nProgramming languages and scheduling algorithms are designed to minimize the over-all effect of blocking. A process that blocks may prevent local work-tasks from progressing. In this case \"blocking\" often is seen as not wanted. However, such work-tasks may instead have been assigned to independent processes, where halting one has little to no effect on the others, since scheduling will continue. An example is \"blocking on a channel\" where passively waiting for the other part (i.e. no polling or spin loop) is part of the semantics of channels. Correctly engineered, any of these may be used to implement reactive systems.\nDeadlock means that processes pathologically wait for each other in a circle. As such it is not directly associated with blocking.\nSee also\n*Concurrent computing\n*Data dependency\n*Non-blocking algorithm\n*Race condition\n*Scheduling (computing)\nReferences"
    }
  ]
}