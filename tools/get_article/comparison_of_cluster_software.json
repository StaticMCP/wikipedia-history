{
  "content": [
    {
      "type": "text",
      "text": "# Comparison of cluster software\n\nThe following tables compare general and technical information for notable computer cluster software. This software can be grossly separated in four categories: Job scheduler, nodes management, nodes installation and integrated stack (all the above).\nGeneral information\n{| class=\"wikitable sortable sort-under\"\n|-\n! Software\n! Maintainer\n! Category\n! Development status\n! Latest release\n! ArchitectureOCS\n! High-Performance / High-Throughput Computing\n! License\n! Platforms supported\n! Cost\n!\n|-\n!  | Amoeba\n|\n|\n|  active development\n|\n|\n|\n|\n|\n|\n|\n|-\n!  | Base One Foundation Component Library\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|-\n!  class=\"table-rh\" | DIET\n| INRIA, SysFera, Open Source\n| All in one\n|\n|\n| GridRPC, SPMD, Hierarchical and distributed architecture, CORBA\n| HTC/HPC\n|\n| Unix-like, Mac OS X, AIX\n|\n|\n|-\n!  | [https://dh2i.com/dxenterprise DxEnterprise]\n| [https://dh2i.com DH2i]\n| Nodes management\n|  developed\n| v23.0\n|\n|\n|\n| Windows 2012R2/2016/2019/2022 and 8+, RHEL 7/8/9, CentOS 7, Ubuntu 16.04/18.04/20.04/22.04, SLES 15.4\n|Cost\n|\n|-\n!  class=\"table-rh\" | Enduro/X\n| Mavimax, Ltd.\n| Job/Data Scheduler\n|  developed\n|\n| SOA Grid\n| HTC/HPC/HA\n| GPLv2 or Commercial\n| Linux, FreeBSD, MacOS, Solaris, AIX\n| Free / Cost\n|\n|-\n!  class=\"table-rh\" | Ganglia\n|\n| Monitoring\n|  developed\n| }}\n|\n|\n|\n| Unix, Linux, Microsoft Windows NT/XP/2000/2003/2008, FreeBSD, NetBSD, OpenBSD, DragonflyBSD, Mac OS X, Solaris, AIX, IRIX, Tru64, HPUX.\n|\n|\n|-\n!  | Grid MP\n| Univa (formerly United Devices)\n| Job Scheduler\n|  active development\n|\n| Distributed master/worker\n| HTC/HPC\n|\n| Windows, Linux, Mac OS X, Solaris\n|\n|\n|-\n!  class=\"table-rh\" | Apache Mesos\n| Apache\n|\n|  developed\n|\n|\n|\n|\n| Linux\n|\n|\n|-\n!  class=\"table-rh\" | Moab Cluster Suite\n| Adaptive Computing\n| Job Scheduler\n|  developed\n|\n|\n| HPC\n|\n| Linux, Mac OS X, Windows, AIX, OSF/Tru-64, Solaris, HP-UX, IRIX, FreeBSD & other UNIX platforms\n|\n|\n|-\n!  class=\"table-rh\" | NetworkComputer\n| Runtime Design Automation\n|\n|  developed\n|\n|\n| HTC/HPC\n|\n| Unix-like, Windows\n|\n|\n|-\n!  class=\"table-rh\" | OpenClusterScheduler\n| Open Cluster Scheduler\n| all in one\n|  developed\n| 9.0.8\n|\n| HTC/HPC\n| SISSL / Apache License\n| Linux (distribution independent / CentOS 7 to Ubuntu 24.04), FreeBSD, Solaris\n|\n|\n|-\n!  class=\"table-rh\" | OpenHPC\n| OpenHPC project\n| all in one\n|  developed\n| v2.61\n|\n| HPC\n|\n| Linux (CentOS / OpenSUSE Leap)\n|\n|\n|-\n!  class=\"table-rh\" | OpenLava\n|  Formerly Teraproc\n| Job Scheduler\n| Halted by injunction\n|\n| Master/Worker, multiple admin/submit nodes\n| HTC/HPC\n| Illegal due to being a pirated version of IBM Spectrum LSF\n| Linux\n|\n|\n|-\n!  class=\"table-rh\" | PBS Pro\n| Altair\n| Job Scheduler\n|  developed\n|\n| Master/worker distributed with fail-over\n| HPC/HTC\n| AGPL or Proprietary\n| Linux, Windows\n|  or Cost\n|\n|-\n!  class=\"table-rh\" | Proxmox Virtual Environment\n| Proxmox Server Solutions\n| Complete\n|  developed\n|\n|\n|\n|\n| Linux, Windows, other operating systems are known to work and are community supported\n|\n|\n|-\n!  | Rocks Cluster Distribution\n| Open Source/NSF grant\n| All in one\n|  developed\n| (Manzanita) }}\n|\n| HTC/HPC\n|\n| CentOS\n|\n|\n|-\n!  | Popular Power\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|-\n!  | ProActive\n| INRIA, ActiveEon, Open Source\n| All in one\n|  developed\n|\n| Master/Worker, SPMD, Distributed Component Model, Skeletons\n| HTC/HPC\n|\n| Unix-like, Windows, Mac OS X\n|\n|\n|-\n!  | RPyC\n| Tomer Filiba\n|\n|  developed\n|\n|\n|\n|\n| *nix/Windows\n|\n|\n|-\n!  | SLURM\n| SchedMD\n| Job Scheduler\n|  developed\n| v23.11.3\n|\n| HPC/HTC\n|\n| Linux/*nix\n|\n|\n|-\n!  class=\"table-rh\" | Spectrum LSF\n| IBM\n| Job Scheduler\n|  developed\n|\n| Master node with failover/exec clients, multiple admin/submit nodes, Suite addOns\n| HPC/HTC\n|\n| Unix, Linux, Windows\n|  and Academic - model - Academic, Express, Standard, Advanced and Suites\n|\n|-\n!  | Oracle Grid Engine (Sun Grid Engine, SGE)\n| Altair\n| Job Scheduler\n| active Development moved to Altair Grid Engine\n|\n| Master node/exec clients, multiple admin/submit nodes\n| HPC/HTC\n|\n| *nix/Windows\n|\n|\n|-\n!  | Some Grid Engine / Son of Grid Engine / Sun Grid Engine\n| daimh\n| Job Scheduler\n|  developed (stable/maintenance)\n|\n| Master node/exec clients, multiple admin/submit nodes\n| HPC/HTC\n|\n| *nix\n|\n|\n|-\n!  | SynfiniWay\n| Fujitsu\n|\n|  developed\n|\n|\n| HPC/HTC\n|\n| Unix, Linux, Windows\n|\n|\n|-\n!  class=\"table-rh\" | Techila Distributed Computing Engine\n| [https://www.techilatechnologies.com/ Techila Technologies Ltd.]\n| All in one\n|  developed\n|\n| Master/worker distributed\n| HTC\n|\n| Linux, Windows\n|\n|\n|-\n!  | TORQUE Resource Manager\n| Adaptive Computing\n| Job Scheduler\n|  developed\n|\n|\n|\n|\n| Linux, *nix\n|\n|\n|-\n!  | TrinityX\n| [https://www.clustervision.com/ ClusterVision]\n| All in one\n|  developed\n| v15\n|\n| HPC/HTC\n|  v3\n| Linux/*nix\n|\n|\n|-\n!  class=\"table-rh\" | UniCluster\n| Univa\n| All in One\n| Functionality and development moved to UniCloud (see above)\n|\n|\n|\n|\n|\n|\n|\n|-\n!  | UNICORE\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|-\n!  | Xgrid\n| Apple Computer\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|-\n!  | Warewulf\n|\n|Provision and clusters management\n|  developed\n|v4.6.4\n|\n|HPC\n|\n|Linux\n|\n|\n|-\n!  | xCAT\n|\n|Provision and clusters management\n|  developed\n|v2.17.0\n|\n|HPC\n| Eclipse Public License\n|Linux\n|\n|\n|-\n! Software\n! Maintainer\n! Category\n! Development status\n!Latest release\n! Architecture\n! High-Performance/ High-Throughput Computing\n! License\n! Platforms supported\n! Cost\n!\n|}\nTable explanation\n* Software: The name of the application that is described\nTechnical information\n{| class=\"wikitable sortable sort-under\"\n! Software\n! Implementation Language\n! Authentication\n! Encryption\n! Integrity\n! Global File System\n! Global File System + Kerberos\n! Heterogeneous/ Homogeneous exec node\n! Jobs priority\n! Group priority\n! Queue type\n! SMP aware\n! Max exec node\n! Max job submitted\n! CPU scavenging\n! Parallel job\n! Job checkpointing\n!\n|-\n!  class=\"table-rh\" | Enduro/X\n| C/C++\n| OS Authentication\n| GPG, AES-128, SHA1\n|\n|  cluster Posix FS (gfs, gpfs, ocfs, etc.)\n|  cluster Posix FS (gfs, gpfs, ocfs, etc.)\n| Heterogeneous\n| OS Nice level\n| OS Nice level\n| SOA Queues, FIFO\n|\n| OS Limits\n| OS Limits\n|\n|\n|\n|\n|-\n!  class=\"table-rh\" | HTCondor\n| C++\n| GSI, SSL, Kerberos, Password, File System, Remote File System, Windows, Claim To Be, Anonymous\n| None, Triple DES, BLOWFISH\n| None, MD5\n| None, NFS, AFS\n|\n| Heterogeneous\n|\n|\n| Fair-share with some programmability\n| basic (hard separation into different node)\n| tested ~10000?\n| tested ~100000?\n|\n| MPI, OpenMP, PVM\n|\n|\n|-\n!  class=\"table-rh\" | PBS Pro\n| C/Python\n| OS Authentication, Munge\n|\n|\n| , e.g., NFS, Lustre, GPFS, AFS\n| Limited availability\n| Heterogeneous\n|\n|\n| Fully configurable\n|\n| tested ~50,000\n| Millions\n|\n| MPI, OpenMP\n|\n|\n|-\n!  class=\"table-rh\" | OpenLava\n| C/C++\n| OS authentication\n|\n|\n| NFS\n|\n| Heterogeneous Linux\n|\n|\n| Configurable\n|\n|\n|\n| , supports preemption based on priority\n|\n|\n|\n|-\n!  class=\"table-rh\" | Slurm\n| C\n| Munge, None, Kerberos\n|\n|\n|\n|\n| Heterogeneous\n|\n|\n| Multifactor Fair-share\n|\n| tested 120k\n| tested 100k\n|\n|\n|\n|\n|-\n!  class=\"table-rh\" | Spectrum LSF\n| C/C++\n| Multiple - OS Authentication/Kerberos\n|\n|\n|  - GPFS/Spectrum Scale, NFS, SMB\n|  - GPFS/Spectrum Scale, NFS, SMB\n| Heterogeneous - HW and OS agnostic (AIX, Linux or Windows)\n| Policy based - no queue to computenode binding\n| Policy based - no queue to computegroup binding\n|  Batch, interactive, checkpointing, parallel and combinations\n|  and GPU aware (GPU License free)\n| > 9.000 compute hots\n| > 4 mio jobs a day\n| , supports preemption based on priority, supports checkpointing/resume\n| , fx parallel submissions for job collaboration over fx MPI\n| , with support for user, kernel or library level checkpointing environments\n|\n|-\n!  | Torque\n| C\n| SSH, munge\n|\n|\n| None, any\n|\n| Heterogeneous\n|\n|\n| Programmable\n|\n| tested\n| tested\n|\n|\n|\n|\n|-\n! Software\n! Implementation Language\n! Authentication\n! Encryption\n! Integrity\n! Global File System\n! Global File System + Kerberos\n! Heterogeneous/ Homogeneous exec node\n! Jobs priority\n! Group priority\n! Queue type\n! SMP aware\n! Max exec node\n! Max job submitted\n! CPU scavenging\n! Parallel job\n! Job checkpointing\n!\n|}\nTable Explanation\n* Software: The name of the application that is described\n* SMP aware:\n** basic: hard split into multiple virtual host\n** basic+: hard split into multiple virtual host with some minimal/incomplete communication between virtual host on the same computer\n** dynamic: split the resource of the computer (CPU/Ram) on demand\nSee also\n* List of volunteer computing projects\n* List of cluster management software\n* Computer cluster\n* Grid computing\n* World Community Grid\n* Distributed computing\n* Distributed resource management\n* High-Throughput Computing\n* Job Processing Cycle\n* Batch processing\n* Fallacies of Distributed Computing\nReferences"
    }
  ]
}